{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Requirments\n",
        "\n",
        "- pandas\n",
        "- numpy\n",
        "- scikit-learn\n",
        "\n",
        "Please run the cell below if they are not installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kto-7NckpcW6",
        "outputId": "5340eb50-f679-4a98-cef4-04c1b83d55aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10370.94s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
            "Requirement already satisfied: pandas in ./ml/lib/python3.11/site-packages (2.2.1)\n",
            "Requirement already satisfied: numpy in ./ml/lib/python3.11/site-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in ./ml/lib/python3.11/site-packages (1.4.1.post1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./ml/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./ml/lib/python3.11/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./ml/lib/python3.11/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in ./ml/lib/python3.11/site-packages (from scikit-learn) (1.12.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in ./ml/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in ./ml/lib/python3.11/site-packages (from scikit-learn) (3.3.0)\n",
            "Requirement already satisfied: six>=1.5 in ./ml/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pandas numpy scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The cell below consists all the `import` statements used. No other libraries are imported further down below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The cell below imports the information about the columns if they are _Continuous_ or _Categorical_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "_lduZU_Uy1NZ"
      },
      "outputs": [],
      "source": [
        "type_df = pd.read_csv('type_info.csv')\n",
        "columns = list(type_df['name'])\n",
        "columns.reverse()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train set and test set are imported.\n",
        "\n",
        "The variables declared here are important to the overall notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('training.data', names=columns)\n",
        "test_df = pd.read_csv('test.data', names=columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleaning the imported data.\n",
        "\n",
        "- Replacing '?' with median for continuous attributes. And middle value for categorical attributes.\n",
        "- Converting values in columns to the appropriate data type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "EJEpRnErulCn"
      },
      "outputs": [],
      "source": [
        "for column in list(type_df[type_df['type']=='Continuous']['name']):\n",
        "  train_df[column] = train_df[column].replace('?', np.nan)\n",
        "  test_df[column] = test_df[column].replace('?', np.nan)\n",
        "  if(train_df[column].dtype == 'object'):\n",
        "    train_df[column] = train_df[column].astype('float64')\n",
        "  if(test_df[column].dtype == 'object'):\n",
        "    test_df[column] = test_df[column].astype('float64')\n",
        "  median_train = train_df[column].median()\n",
        "  train_df[column] = train_df[column].fillna(median_train)\n",
        "  test_df[column] = test_df[column].fillna(median_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "3sdMS1nrv3pJ"
      },
      "outputs": [],
      "source": [
        "for column in list(type_df[type_df['type']=='Categorical']['name']):\n",
        "  unique_train = train_df[column].unique()\n",
        "  unique_train = np.delete(unique_train, np.where(unique_train == '?'))\n",
        "  unique_train.sort()\n",
        "  median_train = unique_train[int(len(unique_train)/2)]\n",
        "  unique_test = test_df[column].unique()\n",
        "  unique_test = np.delete(unique_test, np.where(unique_test == '?'))\n",
        "  unique_test.sort()\n",
        "  train_df[column] = train_df[column].replace('?', np.nan)\n",
        "  test_df[column] = test_df[column].replace('?', np.nan)\n",
        "  train_df[column] = train_df[column].fillna(median_train)\n",
        "  test_df[column] = test_df[column].fillna(median_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Decision Tree Classifier\n",
        "The following class contains all the functions required to calculate `entropy`, `gain`, `gain ratio,` `gini`, `gini index`, appropriately split continuous attributes, compute `f1 score`, perform cross validation. It also includes `get importance` funtion which is required for building a decision tree. `predict` and `fit` functions are also implemented for easier access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "Hl9HRCjhbHTr"
      },
      "outputs": [],
      "source": [
        "class DecisionTreeClassifier():\n",
        "\n",
        "  def __init__(self, train, features, target):\n",
        "      self.__train = train\n",
        "      self.__features = features\n",
        "      self.__target = target\n",
        "      self.__tree = {}\n",
        "      self.__test_pred = []\n",
        "      self.__f1_scores = {}\n",
        "      \n",
        "\n",
        "  def get_agg_f1_scores(self):\n",
        "    if self.__tree == {}:\n",
        "      raise Exception('Please run fit or cross_validate')\n",
        "    if self.__f1_scores == {}:\n",
        "      raise Exception('Please run cross_validate')\n",
        "    return self.__f1_scores\n",
        "  \n",
        "  def get_tree(self):\n",
        "    if self.__tree == {}:\n",
        "      raise Exception('Please run fit or cross_validate')\n",
        "    return self.__tree\n",
        "  \n",
        "  def fit(self, train, features, target, solver='c4.5'):\n",
        "    self.__train = train\n",
        "    self.__features = features\n",
        "    self.__target = target\n",
        "    self.__solver = solver\n",
        "    self.__tree = self.__decision_tree_learning(self.__train, self.__features, self.__train)\n",
        "\n",
        "  def __calculate_entropy(self, attribute_column):\n",
        "    total_count = len(attribute_column)\n",
        "    counts = attribute_column.value_counts()\n",
        "    goals = {}\n",
        "    for item in attribute_column.unique():\n",
        "      goals[item] = counts.at[item]/total_count\n",
        "    entropy_value = 0.0\n",
        "    for goal in goals:\n",
        "      entropy_value -= goals[goal] * math.log(goals[goal], 2)\n",
        "    return entropy_value\n",
        "\n",
        "  def __calculate_gain_ratio(self, train):\n",
        "    total_entropy = self.__calculate_entropy(train[self.__target])\n",
        "    gain = {}\n",
        "    iv = {}\n",
        "    gain_ratio = {}\n",
        "    best_split_gain = {}\n",
        "    best_split_gain_ratio = {}\n",
        "    for feature in self.__features:\n",
        "      if feature in list(type_df[type_df['type'] == 'Continuous']['name']):\n",
        "        best_split_gain[feature], gain[feature], best_split_gain_ratio[feature], gain_ratio[feature], _, _ = self.__split_continuous(train, feature)\n",
        "      else:\n",
        "        attr_unique = train[feature].unique()\n",
        "        gain_att = 0\n",
        "        iv[feature] = 0\n",
        "        for item in attr_unique:\n",
        "          d_ratio = train[feature].value_counts().at[item]/len(train[feature])\n",
        "          gain_att += d_ratio * (self.__calculate_entropy(train[self.__target][train[feature]==item]))\n",
        "          iv[feature] -= d_ratio * math.log(d_ratio, 2)\n",
        "        gain[feature] = total_entropy - gain_att\n",
        "        if(iv[feature] == 0):\n",
        "          gain_ratio[feature] = 0\n",
        "        else:\n",
        "          gain_ratio[feature] = gain[feature]/iv[feature]\n",
        "    return best_split_gain, gain, best_split_gain_ratio, gain_ratio\n",
        "\n",
        "  def __calculate_gini(self, attribute_column):\n",
        "    total_count = len(attribute_column)\n",
        "    counts = attribute_column.value_counts()\n",
        "    goals = {}\n",
        "    for item in attribute_column.unique():\n",
        "      goals[item] = counts.at[item]/total_count\n",
        "\n",
        "    gini = 1.0\n",
        "    for goal in goals:\n",
        "      gini -= goals[goal] * goals[goal]\n",
        "    return gini\n",
        "\n",
        "\n",
        "  def __calculate_gini_index(self, train):\n",
        "    gini_index = {}\n",
        "    best_split_gini_index = {}\n",
        "    for feature in self.__features:\n",
        "      if feature in list(type_df[type_df['type'] == 'Continuous']['name']):\n",
        "        _, _, _, _, best_split_gini_index[feature], gini_index[feature] = self.__split_continuous(train, feature)\n",
        "      else:\n",
        "        attr_unique = train[feature].unique()\n",
        "        gini_att = 0\n",
        "        for item in attr_unique:\n",
        "          gini_att += train[feature].value_counts().at[item]/len(train[feature]) * (self.__calculate_gini(train[self.__target][train[feature]==item]))\n",
        "        gini_index[feature] = gini_att\n",
        "    return best_split_gini_index, gini_index\n",
        "\n",
        "\n",
        "  def __get_importance(self, train, attributes):\n",
        "    best_split_gain, gain_calc, best_split_gain_ratio, gain_ratio_calc = self.__calculate_gain_ratio(train)\n",
        "    best_split_gini_index, gini_index_calc = self.__calculate_gini_index(train)\n",
        "    attributes.sort()\n",
        "    gain_ratio = {}\n",
        "    gini_index = {}\n",
        "    gain = {}\n",
        "    for key in attributes:\n",
        "      gain_ratio[key] = gain_ratio_calc[key]\n",
        "      gini_index[key] = gini_index_calc[key]\n",
        "      gain[key] = gain_calc[key]\n",
        "    if(self.__solver=='c4.5'):\n",
        "      return best_split_gain_ratio, max(gain_ratio, key=gain_ratio.get)\n",
        "    elif(self.__solver=='cart'):\n",
        "      return best_split_gini_index, min(gini_index, key=gini_index.get)\n",
        "    elif(self.__solver=='id3'):\n",
        "      return best_split_gain, max(gain, key=gain.get)\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "  def __split_continuous(self, train, attribute):\n",
        "    sorted_attribute = list(train[attribute].sort_values())\n",
        "    splits = []\n",
        "    for i in range(0, len(sorted_attribute)-1):\n",
        "      splits.append((sorted_attribute[i] + sorted_attribute[i+1])/2)\n",
        "    # print(splits)\n",
        "    split_gain = {}\n",
        "    split_iv = {}\n",
        "    split_gain_ratio = {}\n",
        "    split_gini_index = {}\n",
        "    for split in splits:\n",
        "      d_less = train[train[attribute] <= split][attribute].count()\n",
        "      d_more = train[train[attribute] > split][attribute].count()\n",
        "      split_gain[split] = self.__calculate_entropy(train[self.__target]) - ((d_less * self.__calculate_entropy(train[self.__target][train[attribute] <= split]) + d_more * self.__calculate_entropy(train[self.__target][train[attribute] > split]))/len(train))\n",
        "      if(d_less == 0):\n",
        "        d_less = len(train)\n",
        "      if(d_more == 0):\n",
        "        d_more = len(train)\n",
        "      split_iv[split] = - ((d_less/len(train)) * math.log(d_less/len(train), 2) + (d_more/len(train)) * math.log(d_more/len(train), 2))\n",
        "      if(split_iv[split] == 0):\n",
        "        split_gain_ratio[split] = 0\n",
        "      else:\n",
        "        split_gain_ratio[split] = split_gain[split]/split_iv[split]\n",
        "      split_gini_index[split] = (d_less * self.__calculate_gini(train[self.__target][train[attribute] <= split]) + d_more * self.__calculate_gini(train[self.__target][train[attribute] > split]))/len(train)\n",
        "    max_gain_key = max(split_gain, key=split_gain.get)\n",
        "    max_gain_ratio_key = max(split_gain_ratio, key=split_gain_ratio.get)\n",
        "    min_gini_index_key = min(split_gini_index, key=split_gini_index.get)\n",
        "    return max_gain_key, split_gain[max_gain_key], max_gain_ratio_key, split_gain_ratio[max_gain_ratio_key], min_gini_index_key, split_gini_index[min_gini_index_key]\n",
        "\n",
        "  def get_f1_score(self, test):\n",
        "    if self.__tree == {}:\n",
        "      raise Exception('Please run fit or cross_validate')\n",
        "    test_features = test.drop('A16', axis=1)\n",
        "    y_pred = self.predict(test_features)\n",
        "    y_true = list(test['A16'])\n",
        "    return f1_score(y_true, y_pred, pos_label='+')\n",
        "\n",
        "  def cross_validate(self, train = train_df, splits=10, solver='c4.5'):\n",
        "    self.__f1_scores[solver] = {}\n",
        "    kf = KFold(n_splits=splits)\n",
        "    kf.get_n_splits(train_df)\n",
        "    for i, (train_index, test_index) in enumerate(kf.split(train_df)):\n",
        "        train_set = train.iloc[train_index]\n",
        "        test_set = train.iloc[test_index]\n",
        "        self.fit(train_set, self.__features, self.__target, solver=solver)\n",
        "        self.__f1_scores[solver][i] = self.get_f1_score(test_set)\n",
        "    return self.__f1_scores[solver]\n",
        "\n",
        "  def predict(self, test):\n",
        "    self.__test_pred = []\n",
        "    test_dict = test.to_dict(orient='index')\n",
        "    for index, row in test_dict.items():\n",
        "      self.__classify(row, self.__tree)\n",
        "    return self.__test_pred\n",
        "\n",
        "  def __classify(self, row, tree):\n",
        "    key = list(tree)[0]\n",
        "    value = tree[key]\n",
        "    if(key in list(type_df[type_df['type'] == 'Continuous']['name'])):\n",
        "      for item in list(value):\n",
        "        if(eval(str(row[key])+str(item))):\n",
        "          branch = item\n",
        "          if type(value[branch]) is not dict:\n",
        "            self.__test_pred.append(value[branch])\n",
        "          else:\n",
        "            self.__classify(row, value[branch])\n",
        "    else:\n",
        "      branch = row[key]\n",
        "      key_list = list(value)\n",
        "      if branch in key_list:\n",
        "        branch = branch\n",
        "      else:\n",
        "        branch = random.choice(key_list)\n",
        "      if type(value[branch]) is not dict:\n",
        "          self.__test_pred.append(value[branch])\n",
        "      else:\n",
        "        self.__classify(row, value[branch])\n",
        "\n",
        "  def __decision_tree_learning(self, examples, attributes, parent_examples):\n",
        "    tree = {}\n",
        "    if len(examples[self.__target]) == 0:\n",
        "      return parent_examples[self.__target].value_counts().index[0]\n",
        "    elif len(examples[self.__target].unique()) == 1:\n",
        "      return examples[self.__target].unique()[0]\n",
        "    elif len(attributes) == 0:\n",
        "      return examples[self.__target].value_counts().index[0]\n",
        "    else:\n",
        "      best_split, best_attr = self.__get_importance(examples, attributes)\n",
        "      tree[best_attr] = {}\n",
        "      if best_attr in list(type_df[type_df['type'] == 'Continuous']['name']):\n",
        "        for i in ['<=' , '>']:\n",
        "          exs = examples.query(f'{best_attr} {i} {best_split[best_attr]}')\n",
        "          subtree = self.__decision_tree_learning(exs, list(set(attributes)-set([best_attr])), examples)\n",
        "          tree[best_attr][f'{i} {str(best_split[best_attr])}'] = subtree\n",
        "      else:\n",
        "        unique_train_best_attr = self.__train[best_attr].unique()\n",
        "        unique_train_best_attr.sort()\n",
        "        for att_val in unique_train_best_attr:\n",
        "          exs = examples[examples[best_attr] == att_val]\n",
        "          subtree = self.__decision_tree_learning(exs, list(set(attributes)-set([best_attr])), examples)\n",
        "          tree[best_attr][att_val] = subtree\n",
        "      return tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialization of `DecisionTreeClassifier` class takes training data set (`train_df`) which was imported and cleaned earlier. And it also takes a list of `feature` attributes, and the name of the `target` attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "mq-ICQOKdJ3K"
      },
      "outputs": [],
      "source": [
        "train_columns = list(train_df.columns)\n",
        "train_target = 'A16'\n",
        "train_features = train_columns.copy()\n",
        "train_features.remove(train_target)\n",
        "\n",
        "dt = DecisionTreeClassifier(train_df, train_features, train_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following cell performs cross-validation with 10 splits. `solvers` list can be modified to include solvers that are required. Currently it performs cross validation on `id3`, `c4.5`, and `cart` and stores them in a class variable. The solver names are case-sensitive. `cross_validate` function calls `fit` function internally. It can also be called separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [],
      "source": [
        "solvers = ['id3', 'c4.5', 'cart']\n",
        "for solver in solvers:\n",
        "    dt.cross_validate(solver=solver)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Access the `f1_score` values computed earlier for all the `solvers`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [],
      "source": [
        "agg_f1_scores = dt.get_agg_f1_scores()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id3': {0: 0.7368421052631579,\n",
              "  1: 0.8979591836734694,\n",
              "  2: 0.64,\n",
              "  3: 0.75,\n",
              "  4: 0.6829268292682927,\n",
              "  5: 0.8333333333333334,\n",
              "  6: 0.7636363636363637,\n",
              "  7: 0.7906976744186046,\n",
              "  8: 0.7307692307692307,\n",
              "  9: 0.75},\n",
              " 'c4.5': {0: 0.7692307692307693,\n",
              "  1: 0.8461538461538461,\n",
              "  2: 0.84,\n",
              "  3: 0.8695652173913043,\n",
              "  4: 0.6976744186046512,\n",
              "  5: 0.8461538461538461,\n",
              "  6: 0.8,\n",
              "  7: 0.8,\n",
              "  8: 0.8363636363636363,\n",
              "  9: 0.7857142857142857},\n",
              " 'cart': {0: 0.7368421052631579,\n",
              "  1: 0.7391304347826086,\n",
              "  2: 0.7659574468085106,\n",
              "  3: 0.6976744186046512,\n",
              "  4: 0.8085106382978723,\n",
              "  5: 0.7843137254901961,\n",
              "  6: 0.7547169811320755,\n",
              "  7: 0.8,\n",
              "  8: 0.72,\n",
              "  9: 0.7037037037037037}}"
            ]
          },
          "execution_count": 160,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agg_f1_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We choose the best split where we have the highest `f1_score` and call `fit` function on that specific train split. `f1_score` values are calculated on test set (`test_df`) provided and printed as the cell output. Currently it performs prediction for all the solvers present in `agg_f1_scores` variable. That can be adjusted by adjusting the `solvers` in the cell where the `cross_validate` function was called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score for id3: 0.8062015503875969\n",
            "F1 Score for c4.5: 0.8\n",
            "F1 Score for cart: 0.784\n"
          ]
        }
      ],
      "source": [
        "kf_best = KFold(n_splits=10)\n",
        "for solver in list(agg_f1_scores):\n",
        "    best_train_indices = []\n",
        "    best_split = max(agg_f1_scores[solver], key=agg_f1_scores[solver].get)\n",
        "    for i, (train_index, _) in enumerate(kf_best.split(train_df)):\n",
        "        if i == best_split:\n",
        "            train_set = train_df.iloc[train_index]\n",
        "    dt.fit(train_set, train_features, train_target, solver)\n",
        "    print(f'F1 Score for {solver}: {dt.get_f1_score(test_df)}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
